# -*- coding: utf-8 -*-
"""nn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zzrs2KuFbOMy3_GZ8_DfUQAcFo-ib_UU
"""

import torch
import torch .nn as nn
import torch . optim as optim
from torch . utils . data import DataLoader
import torchvision . transforms as transforms
import torchvision . datasets as datasets

!pip install datasets

!pip install -U datasets fsspec

!pip install pandas

import pandas as pd

url = "hf://datasets/Zhouhc/attack-agnews/agnews_train.csv"
df = pd.read_csv(url)

# Display the first few rows of the dataset
print(df.head())

df.head(1000000)      # Shows the first 10 rows

!pip install -U sentence-transformers

df_sample = df.sample(frac=0.1, random_state=42).reset_index(drop=True)
texts = df_sample['text'].tolist()

!pip install -U sentence-transformers
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(texts, show_progress_bar=True)

import torch
import torch.nn as nn

class Encoder(nn.Module):
    def __init__(self, input_size=384, hidden_size=128, embedding_size=64):
        super(Encoder, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, embedding_size)
        )

    def forward(self, x):
        return self.net(x)

from torch.utils.data import DataLoader, TensorDataset

# Convert to tensor
embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)

# Create DataLoader
dataset = TensorDataset(embeddings_tensor)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = Encoder().to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.MSELoss()  # reconstruction loss (as a placeholder)

# Dummy target = input (like autoencoder)
for epoch in range(5):
    model.train()
    total_loss = 0
    for batch in loader:
        x = batch[0].to(device)
        optimizer.zero_grad()
        output = model(x)
        loss = loss_fn(output, x[:, :output.shape[1]])  # use input as target
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {total_loss:.4f}")

model.eval()
with torch.no_grad():
    latent = model(embeddings_tensor.to(device)).cpu().numpy()

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score

kmeans = KMeans(n_clusters=4, random_state=42)
cluster_labels = kmeans.fit_predict(latent)

# Evaluation
print("Silhouette Score:", silhouette_score(latent, cluster_labels))
print("Davies-Bouldin Index:", davies_bouldin_score(latent, cluster_labels))

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

tsne = TSNE(n_components=2, random_state=42)
proj = tsne.fit_transform(latent)

plt.scatter(proj[:, 0], proj[:, 1], c=cluster_labels, cmap='tab10', s=10)
plt.title("t-SNE Visualization of Clusters")
plt.colorbar()
plt.show()